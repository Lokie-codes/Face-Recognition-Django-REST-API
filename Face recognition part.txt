Face recognition part
I. Face Detection
II. Face Recognition

I. Face Detection
Involves face localisation - estimating the face bounding boxes without any scale and position prior.[1] 

1) Fetch Image - Image is obtained from the client through the POST request.
2) Pre-process image
	resizing - upscaling or downscaling image to standardize  input.
	The resized image is then cast to a numpy array of dtype float32.
	Conversion of BGR to RGB color channels
	transpose (N,H,W,C) to (N,C,H,W) 
	where 	N: number of images in the batch
		H: height of the image
		W: width of the image
		C: number of channels of the image (ex: 3 for RGB, 1 for grayscale...)
	store these values in an image tensor
3) Detect Faces
	Image Tensor is passed to a pre-trained Faster R-CNN (Region-based Convolutional Neural Network) model.
	This model produces predicted scores and bounding box deltas.
	This is Iterated over feature strides corresponding to different scale level in FPN(Feature Pyramid Network)
	Number of anchors are generated from feature stride which are used to reshape the predictions into a set of proposals and their associated scores.
	Proposals are generated by applying the bounding box deltas to the anchors and these are clipped to the image boundary.
	 Decay the scores of proposals with a stride size of 4. The reason for this is that anchors with a stride size of 4 have a higher overlap with neighboring anchors, which can lead to redundant detections. Decaying the scores of proposals with a stride size of 4 can help reduce the number of redundant detections.
	 Next, the proposals are filtered based on their score, with only proposals whose score is greater than or equal to a given threshold being kept.	
	 The remaining proposals are then divided by the image scale and added to a list of proposals, along with their associated scores and landmarks. The landmarks are predicted offsets from the anchors, and are also divided by the image scale.
	 Postprocessing where the box bounding of the faces from the proposals happen and the identifying features are coupled in a dictionary. This is passed to the face recognition.


II. Face Recognition

1) Alignment
i) 2D Alignment
	6 Fudicial points are detected namely, eye centers, nose tip and mouth locations which are used to scale, rotate and translate the image.
	Iterate the new warped image until no substantial change is obtained.
	This generates a 2D aligned crop useful in boosting recognition accuracy.
ii) 3D Alignment
	Performed by using a generic 3D shape model and registering a 3D affine camera for 3D alignment of faces undergoing out-of-plane rotations. 
	67 fiducial points are localized in the 2D-aligned crop using a second SVR, and a full correspondence between the detected fiducial points and their 3D references is achieved by manually placing 67 anchor points on the 3D shape. 
	An affine 3D-to-2D camera is fitted using the generalized least squares solution to the linear system, with a known covariance matrix. The loss function is minimized using the Cholesky decomposition of the covariance matrix. The method can handle noisy fiducial points, as the covariance matrix is given by the estimated covariances of the fiducial point errors.
iii) frontalization
	Aims to align and warp 2D images of faces to a 3D generic shape model in a way that minimizes distortions to the identity. 
	The method involves localizing 67 fiducial points in the 2D image using a second Support Vector Regression (SVR) and fitting an affine 3D-to-2D camera P using a generalized least squares solution. 
	The camera is then used to warp the 2D-aligned crop to the image plane of the 3D shape, generating the 3D-aligned version of the crop. 
	However, since the fitted camera P is only an approximation, the corresponding residuals in r are added to the x-y components of each reference fiducial point x3d to reduce the corruption of identity-bearing factors. 
	Finally, the frontalization is achieved using a piece-wise affine transformation T from x2d to xf3d, directed by the Delaunay triangulation derived from the 67 fiducial points. 
	Additionally, invisible triangles with respect to camera P can be replaced using image blending with their symmetrical counterparts.
	
2) Representation
	Convolutional neural network (CNN) is used for face recognition. 
	The input to the CNN is a 3D-aligned face image with RGB channels of size 152 x 152 pixels. The CNN consists of three layers - two convolutional layers (C1 and C3) and one max-pooling layer (M2).
	The first layer, C1, has 32 filters of size 11x11x3, denoted by 32x11x11x3@152x152. This means that each filter is of size 11x11 and takes a 3D input with 3 channels (RGB). The output of this layer is a set of 32 feature maps.
	The second layer, M2, is a max-pooling layer that takes the max over 3x3 spatial neighborhoods with a stride of 2, separately for each channel. This reduces the spatial size of the feature maps and increases their depth. The purpose of max-pooling is to make the network more robust to local translations, such as small registration errors in face alignment.
	The third layer, C3, has 16 filters of size 9x9x16. The input to this layer is the set of 32 feature maps generated by the first layer. The purpose of this layer is to extract low-level features, such as simple edges and texture.
	It is important to note that max-pooling is only applied to the first convolutional layer. This is because several levels of pooling would cause the network to lose information about the precise position of detailed facial structure and micro-textures. 
	Therefore, the first layers are interpreted as a front-end adaptive pre-processing stage that extracts low-level features and reduces the spatial size of the input while preserving important facial structure details.
	The subsequent layers (L4, L5 and L6) are locally connected, meaning that every location in the feature map learns a different set of filters. 
	This is because different regions of an aligned image have different local statistics, and the spatial stationarity assumption of convolution cannot hold.
	The top two layers (F7 and F8) are fully connected, allowing them to capture correlations between features captured in distant parts of the face images. 
	The output of the first fully connected layer (F7) is used as the raw face representation feature vector. The output of the last fully-connected layer is fed to a K-way softmax, which produces a distribution over the class labels. 
	The probability assigned to the k-th class is calculated using the softmax function, which normalizes the outputs of the last layer and converts them into a probability distribution.

3) Verification Metric
	The verification metric is used to determine whether two input instances belong to the same class (identity) or not. 
	The unsupervised similarity is determined by the inner product between the two normalized feature vectors. 
	The metric used is the weighted-χ^2 similarity.
	The weighted-χ2 similarity is a measure of similarity between two feature vectors, f1 and f2, that contain non-negative, sparse, and normalized values. It is based on the χ2 distance, which is commonly used to measure the distance between histograms. The weighted-χ2 similarity is computed by summing up the weighted square differences between each element in the two feature vectors, normalized by their sum, as follows:
χ2(f1, f2) = Σi wi(f1[i] − f2[i])^2 / (f1[i] + f2[i])
Here, wi is a weight parameter that is learned using a linear Support Vector Machine (SVM) on a set of vectors of the form (f1[i] − f2[i])^2 / (f1[i] + f2[i]). The SVM learns to classify pairs of feature vectors as similar or dissimilar, based on the weighted-χ2 similarity measure, and the weight parameters are adjusted to optimize the classification performance. The weighted-χ2 similarity has been shown to be effective in measuring similarity between sparse and normalized feature vectors.
